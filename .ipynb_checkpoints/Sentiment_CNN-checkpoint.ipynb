{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Analysis of Movie Reviews\n",
    "===================================\n",
    "\n",
    "The IMDB dataset consists of 25,000 reviews, each with a binary label (1 = positive, 0 = negative). Here is an example review:\n",
    "\n",
    "> “Okay, sorry, but I loved this movie. I just love the whole 80’s genre of these kind of movies, because you don’t see many like this...” -~CupidGrl~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "from os import listdir\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "# split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "# prepare regex for char filtering\n",
    "    re_punc = re.compile( ' [%s] ' % re.escape(string.punctuation))\n",
    "# remove punctuation from each word\n",
    "    tokens = [re_punc.sub( '' , w) for w in tokens]\n",
    "# remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "# filter out stop words\n",
    "    stop_words = set(stopwords.words( 'english' ))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "# filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r',encoding=\"utf8\")\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'film', 'lot', 'plot', 'relatively', 'however', 'director', 'editors', 'seriously', 'let', 'film', 'feel', 'bad', 'could', 'The', 'acting', 'characters', 'ever', 'edited', 'clearly', 'learnt', 'new', 'edit', 'techniques', 'wanted', 'splash', 'There', 'lots', 'quick', 'edits', 'almost', 'every', 'clearly', 'meant', 'symbolic', 'end', 'wanted', 'like', 'film', 'expected', 'decent', 'resolution', 'breakdown', 'equilibrium', 'alas', 'left', 'feeling', 'like', 'wasted', 'time', 'film', 'makers', 'wasted']\n"
     ]
    }
   ],
   "source": [
    "# load the document\n",
    "filename = 'C:/Users/GCNDP/SentimentTrain/train/neg/10_2.txt'\n",
    "text = load_doc(filename)\n",
    "tokens = clean_doc(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc and add to vocab\n",
    "def add_doc_to_vocab(filename, vocab):\n",
    "# load doc\n",
    "    doc = load_doc(filename) #encoding=\"utf8\"\n",
    "# clean doc\n",
    "    tokens = clean_doc(doc)\n",
    "# update counts\n",
    "    vocab.update(tokens)\n",
    "    #print(tokens)\n",
    "\n",
    "# load all docs in a directory\n",
    "def process_docs_to_vocab(directory, vocab):\n",
    "# walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "# create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "# add doc to vocab\n",
    "        #print(path)\n",
    "        add_doc_to_vocab(path, vocab)\n",
    "        \n",
    "# turn a doc into clean tokens\n",
    "def clean_doc_wVocab(doc, vocab):\n",
    "\t# split into tokens by white space\n",
    "\ttokens = doc.split()\n",
    "\t# prepare regex for char filtering\n",
    "\tre_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\t# remove punctuation from each word\n",
    "\ttokens = [re_punc.sub('', w) for w in tokens]\n",
    "\t# filter out tokens not in vocab\n",
    "\ttokens = [w for w in tokens if w in vocab]\n",
    "\ttokens = ' '.join(tokens)\n",
    "\treturn tokens\n",
    "\n",
    "# load all docs in a directory, into tokens\n",
    "def process_docs_to_tokens(directory, vocab):\n",
    "\tdocuments = list()\n",
    "\t# walk through all files in the folder\n",
    "\tfor filename in listdir(directory):\n",
    "\t\t# create the full path of the file to open\n",
    "\t\tpath = directory + '/' + filename\n",
    "\t\t# load the doc\n",
    "\t\tdoc = load_doc(path)\n",
    "\t\t# clean doc\n",
    "\t\ttokens = clean_doc_wVocab(doc, vocab)\n",
    "\t\t# add to list\n",
    "\t\tdocuments.append(tokens)\n",
    "\treturn documents\n",
    "\n",
    "# integer encode and pad documents\n",
    "def encode_docs(tokenizer, max_length, docs):\n",
    "    # integer encode\n",
    "    encoded = tokenizer.texts_to_sequences(docs)\n",
    "    # pad sequences\n",
    "    padded = pad_sequences(encoded, maxlen=max_length, padding='post')\n",
    "    return padded\n",
    "\n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "\ttokenizer = Tokenizer()\n",
    "\ttokenizer.fit_on_texts(lines)\n",
    "\treturn tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75355\n",
      "[('The', 33762), ('movie', 30506), ('film', 27402), ('one', 20692), ('like', 18133), ('This', 12279), ('would', 11923), ('good', 11436), ('It', 10952), ('really', 10815), ('even', 10607), ('see', 10155), ('get', 8777), ('story', 8527), ('much', 8507), ('time', 7765), ('make', 7485), ('could', 7462), ('also', 7422), ('first', 7339), ('people', 7335), ('great', 7191), ('made', 6962), ('think', 6659), ('bad', 6506), ('many', 6062), ('never', 6043), ('But', 5897), ('two', 5869), ('little', 5790), ('way', 5649), ('And', 5590), ('well', 5420), ('watch', 5314), ('seen', 5304), ('know', 5270), ('character', 5215), ('characters', 5180), ('movies', 5128), ('best', 4975), ('love', 4974), ('ever', 4924), ('still', 4863), ('In', 4788), ('films', 4740), ('plot', 4698), ('acting', 4648), ('show', 4472), ('He', 4466), ('better', 4406)]\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from collections import Counter\n",
    "\n",
    "# define vocab\n",
    "vocab = Counter()\n",
    "# add all docs to vocab\n",
    "process_docs_to_vocab('C:/Users/GCNDP/SentimentTrain/train/pos', vocab)\n",
    "process_docs_to_vocab('C:/Users/GCNDP/SentimentTrain/train/neg', vocab)\n",
    "# print the size of the vocab\n",
    "print(len(vocab))\n",
    "# print the top words in the vocab\n",
    "print(vocab.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46000\n"
     ]
    }
   ],
   "source": [
    "# save list to file\n",
    "def save_list(lines, filename):\n",
    "    # convert lines to a single blob of text\n",
    "    data = '\\n'.join(lines)\n",
    "    # open file\n",
    "    file = open(filename, 'w', encoding='utf-8')\n",
    "    # write text\n",
    "    file.write(data)\n",
    "    # close file\n",
    "    file.close()\n",
    "    \n",
    "min_occurane = 2\n",
    "tokens = [k for k,c in vocab.items() if c >= min_occurane]\n",
    "print(len(tokens))\n",
    "# save tokens to a vocabulary file\n",
    "save_list(tokens, \"C:/Users/GCNDP/SentimentTrain/vocab.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the vocabulary\n",
    "vocab_filename = 'C:/Users/GCNDP/SentimentTrain/vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = set(vocab.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and clean a dataset\n",
    "def load_clean_dataset(vocab):\n",
    "\t# load documents\n",
    "\tneg = process_docs_to_tokens('C:/Users/GCNDP/SentimentTrain/train/neg', vocab)\n",
    "\tpos = process_docs_to_tokens('C:/Users/GCNDP/SentimentTrain/train/pos', vocab)\n",
    "\tdocs = neg + pos\n",
    "\t# prepare labels\n",
    "\tlabels = array([0 for _ in range(len(neg))] + [1 for _ in range(len(pos))])\n",
    "\treturn docs, labels\n",
    "\n",
    "# load and clean a dataset\n",
    "def load_clean_dataset_test(vocab):\n",
    "\t# load documents\n",
    "\tneg = process_docs_to_tokens('C:/Users/GCNDP/SentimentTrain/test/neg', vocab)\n",
    "\tpos = process_docs_to_tokens('C:/Users/GCNDP/SentimentTrain/test/pos', vocab)\n",
    "\tdocs = neg + pos\n",
    "\t# prepare labels\n",
    "\tlabels = array([0 for _ in range(len(neg))] + [1 for _ in range(len(pos))])\n",
    "\treturn docs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "train_docs, ytrain = load_clean_dataset(vocab)\n",
    "# create the tokenizer\n",
    "tokenizer = create_tokenizer(train_docs)\n",
    "# define vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary size: %d' % vocab_size)\n",
    "# calculate the maximum sequence length\n",
    "max_length = max([len(s.split()) for s in train_docs])\n",
    "print('Maximum length: %d' % max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = encode_docs(tokenizer, max_length, train_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 1406)\n",
      "[  9 631  37 ...   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "print(Xtrain.shape)\n",
    "print(Xtrain[5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "# define the model\n",
    "def define_model(vocab_size, max_length):\n",
    "    model = Sequential()\n",
    "    model.add(layers.Embedding(vocab_size, 100, input_length=max_length))\n",
    "    model.add(layers.Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    "    model.add(layers.MaxPooling1D(pool_size=2))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(10, activation='relu' ))\n",
    "    model.add(layers.Dense(1, activation='sigmoid' ))\n",
    "# compile network\n",
    "    model.compile(loss='binary_crossentropy' , optimizer=optimizers.Adam() , metrics=['acc'])\n",
    "# summarize defined model\n",
    "    model.summary()\n",
    "    plot_model(model, to_file= 'C:/users/GCNDP/model.png' , show_shapes=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38428\n",
      "1406\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 1406, 100)         3842800   \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1399, 32)          25632     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 699, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 22368)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                223690    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 4,092,133\n",
      "Trainable params: 4,092,133\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      " - 30s - loss: 0.4910 - acc: 0.7161\n",
      "Epoch 2/10\n",
      " - 30s - loss: 0.1795 - acc: 0.9326\n",
      "Epoch 3/10\n",
      " - 29s - loss: 0.0605 - acc: 0.9804\n",
      "Epoch 4/10\n",
      " - 29s - loss: 0.0155 - acc: 0.9959\n",
      "Epoch 5/10\n",
      " - 29s - loss: 0.0031 - acc: 0.9994\n",
      "Epoch 6/10\n",
      " - 29s - loss: 7.6758e-04 - acc: 0.9999\n",
      "Epoch 7/10\n",
      " - 29s - loss: 0.0010 - acc: 0.9998\n",
      "Epoch 8/10\n",
      " - 29s - loss: 0.0228 - acc: 0.9933\n",
      "Epoch 9/10\n",
      " - 29s - loss: 0.0100 - acc: 0.9965\n",
      "Epoch 10/10\n",
      " - 29s - loss: 0.0011 - acc: 0.9996\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1375a668>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(vocab_size)\n",
    "print(max_length)\n",
    "# define model\n",
    "model = define_model(vocab_size, max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit network\n",
    "model.fit(Xtrain, ytrain, epochs=10, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('C:/Users/GCNDP/SentimentTrain/sent_model_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import models\n",
    "model = models.load_model('C:/Users/GCNDP/SentimentTrain/sent_model_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docs, ytest = load_clean_dataset_test(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest = encode_docs(tokenizer, max_length, test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train Accuracy: 100.000000 \n",
      " Test Accuracy: 85.080000 \n"
     ]
    }
   ],
   "source": [
    "# evaluate model on training dataset\n",
    "_, acc = model.evaluate(Xtrain, ytrain, verbose=0)\n",
    "print( ' Train Accuracy: %f ' % (acc*100))\n",
    "# evaluate model on test dataset\n",
    "_, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
    "print( ' Test Accuracy: %f ' % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = \"the movie had a good plot to it, and the actors did a magnificent job. Overall, i would recomend it.\";\n",
    "#\"The move is enjoyable. Recommended for all ages. The storyline is good and direction is good\";\n",
    "#log, and incongruous to the film. As for the story, it was a bit preachy and militant in tone. Overall, I was disappointed, but I would go again just to see the same excitement on my child's face. I liked Lumpy's laugh...\";\n",
    "#\"The move is enjoyable. Recommended for all ages. The storyline is good and direction is good\"\n",
    "#\"The movie started good. But after half-time, the story line faded, there was too much theatrical element. Not recommended\"\n",
    "#\"The characters voices were very good. I was only really bothered by Kanga. The music, however, was twice as loud in parts than the dialog, and incongruous to the film. As for the story, it was a bit preachy and militant in tone. Overall, I was disappointed, but I would go again just to see the same excitement on my child's face. I liked Lumpy's laugh...\"\n",
    "#\"Beautiful attracts excellent idea, but ruined with a bad selection of the actors. The main character is a loser and his woman friend and his friend upset viewers. Apart from the first episode all the other become more boring and boring. First, it considers it illogical behavior. No one normal would not behave the way the main character behaves. It all represents a typical Halmark way to endear viewers to the reduced amount of intelligence. Does such a scenario, or the casting director and destroy this question is on Halmark producers. Cat is the main character is wonderful. The main character behaves according to his friend selfish.\"\n",
    "#\"The pace is steady and constant, the characters full and engaging, the relationships and interactions natural showing that you do not need floods of tears to show emotion, screams to show fear, shouting to show dispute or violence to show anger. Naturally Joyce's short story lends the film a ready made structure as perfect as a polished diamond, but the small changes Huston makes such as the inclusion of the poem fit in neatly. It is truly a masterpiece of tact, subtlety and overwhelming beauty.\"\n",
    "#'This is a bad movie. Do not watch it. It sucks.'\n",
    "#'Everyone will enjoy this film. I love it, recommended!'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie good plot actors magnificent job Overall would recomend\n",
      "[[ 2  7 43 ...  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "line = clean_doc_wVocab(review, vocab)\n",
    "print(line)\n",
    "X_encoded = encode_docs(tokenizer, max_length, [line])\n",
    "print(X_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9391502\n",
      "POSITIVE\n"
     ]
    }
   ],
   "source": [
    "yhat = model.predict(X_encoded, verbose=0)\n",
    "\t# retrieve predicted percentage and label\n",
    "percent_pos = yhat[0,0]\n",
    "print(percent_pos)\n",
    "if round(percent_pos) >= 0.5:\n",
    "\tsentiment = 'POSITIVE'\n",
    "else:\n",
    "\tsentiment = 'NEGATIVE'\n",
    "    \n",
    "print(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
